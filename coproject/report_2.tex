\documentclass[a4paper, 11pt]{article}
\usepackage[english]{babel}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{xcolor}

\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}

\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}

\newcommand{\x}{x_i}
\newcommand{\y}{y_i}

\newcommand{\inner}[2]{\langle #1 , #2 \rangle_{\R^d}}
\newcommand{\norm}[1]{\left\| #1 \right\|}

\title{Kernel Methods and SVMs:
A convex optimization point of view}
\author{Gael Lederrey\\
  \and
Corentin Tallec}

\begin{document}
\maketitle
\begin{abstract}
  Kernel methods, and more specifically support vectors machines, are
  well known, well theorized and efficient machine learning tools. For
  years, they were presenting state of the art performances in many
  machine learning fields, before being overtaken by deep learning
  methods. They have the enormous advantage of being well understood
  theoretically, and most interestingly to belong to the class of
  convex optimization problems, which is clearly not the case of the
  latter.

  In this document, we aim at presenting the general framework of
  Kernel methods, to expose how they relate to convex optimization,
  and how their optimization can be undertaken in practice. Besides,
  we intend to give a more precise treatment of the support vectors
  machines special case.
\end{abstract}

\section{Kernel methods}
\subsection{Generalities}

Kernel methods are tools broadly used in various fields, but most notably in
machine learning and pattern discrimination. Among this class of methods,
support vector machines are probably the best known kernel method. This class
of algorithm is used to find and study some types of relations in datasets,
{\it e.g.} classifications of the data, correlations between them, etc. Many
machine learning algorithms rely on the idea of embedding the data at hand
into a vector space that makes it easier analyse. Kernel methods provide
a quite general framework for doing so, and provide tools to transport data
into possibly infinite dimensional spaces. Kernel methods rely on a measure
of similarity between input data known as a {\bf kernel}, which has to be selected
beforehand.

A typical case of application for a kernel method is the following. Define
$\X$ (resp. $\Y$) as a set of datapoints (resp. a set of labels). Given
a training set, that is a set of couples $(\x, \y)_{i\leq n} \in (\X \times \Y)^n$,
we want to find an $f : \X \mapsto \Y$ in a particular set of functions $\F$ such
that $f$ predicts accurately labels in the training set, and is able to generalize
to unseen datapoints. One way of doing so is to solve the following optimization
problem:
\begin{equation}
  \min\limits_{f\in\F} \frac{1}{n}\sum\limits_{i=1}^nL(\y,f(\x)) + \lambda\Omega(f)
\end{equation}
where $L$ is a cost function, that measure how close $f(\x)$ is to $\y$ for each
$i$, and $\Omega$ is a regularizing term, that aims at describing the complexity
of a certain function $f$. In the general case, this optimization problem is not required
to be convex. Kernel methods provide a set of function $\F$ and a regularizing term
$\Omega(f)$ such that when the cost function is assumed to be convex, the entire
problem is convex, and can be easily solved.

In the following sections, kernel functions are defined, as well as their
relation to the idea of feature mapping, and the general principle of kernel
methods is exposed.

\subsubsection{Positive Definite Kernel}
\label{subsub:psd}
Denote by $\X$ an arbitrary input set (notably $\X$ is not required to be a vector
space). A kernel can be seen as a measure of similarity between two elements of $\X$.
Formally
\begin{definition}
  A positive definite (p.d.) kernel on $\X$ is a function $K:\X\times\X \mapsto \R$ that
  is symmetric and that satisfies for all $N \in \N$, $(x_1, \dots, x_n) \in \X^N$,
  $(a_1, \dots, a_N) \in \R^\N$:
  \begin{equation}
    \sum\limits_{1 \leq i, j \leq N}a_ia_jK(x_i,x_j) \geq 0.
  \end{equation}
\end{definition}
If $\X = R^d$, the simplest kernel that can be thought of
is the canonical inner product. It obviously is symmetric, 
and positivity is easily verified. 
Under the same assumptions, it can quite easily be shown
that $K(x, x') = (\inner{x}{x'})^p$ is a kernel too (known
as the polynomial kernel). Another well known kernel is 
the gaussian kernel, defined as $K(x, x') = e^{- \frac{\norm{x-y}^2}{2\sigma^2}}$.

\subsubsection{Feature mapping}
\label{map}
As seen in Subsection~\ref{subsub:psd}, the canonical inner
product of a vector space is a kernel. Mercer's theorem provides
a kind of reciprocal statement: any kernel can be viewed as an
inner product in a certain hilbert space which is a functional
space on $\mathcal{X}$.
\begin{theorem} [Mercer's theorem]
The function $K$ is a positive definite kernel on $\mathcal{X}$ if and only
if there exists a Hilbert space $\mathcal{H}$ with an inner product $\langle
\cdot,\cdot\rangle_{\mathcal{H}}$ and a mapping
\begin{equation}
  \phi: \mathcal{X}\mapsto\mathcal{H}
\end{equation}
such that, for any $x$, $x'$ in $\mathcal{X}$:
\begin{equation}
  K(x,x') = \langle\phi(x),\phi(x')\rangle_{\mathcal{H}}
\end{equation}
\end{theorem}
Using a kernel can thus somehow be viewed as embedding the datapoints in
a (larger) space. One benefit of doing so can be, for example, to simplify
regression or classification problems, as these problems tend to
be easier in high dimensional space than in low dimensional ones.

\subsubsection{Back to classification}
\subsubsection{Idea about the Kernel Methods}
\label{meth}
The {\bf kernel methods} are using a set of pre-trained data. We can denote them by the pairs of example: $(x_1, y_1), \ldots, (x_n,y_n) \in \mathcal{X} \times \mathcal{Y}$. This means that for all the input $x_i, ~ i=1,\ldots,n$, we know the corresponding result $y_i$. Now, we can add an unlabelled value, let's say $x'$. The kernel function will be used to compare $x'$ with all the pre-trained inputs $x_i$. The kernel will then say if $x'$ is similar to $x_i$ or not. With adding weights for each of the pre-trained examples, an output $\hat{y}$ depending on the unlabelled input $x'$ can be computed. 
\\ 
We can use the example of a {\it kernelized binary classifier}. It computes a weighted sum of similarities:
\begin{equation}
\hat{y} = \text{sgn}\sum_{i=1}^{n }w_i y_i K(x_i,x')
\end{equation}
where
\begin{itemize}
\item $\hat{y}\in\{-1,+1\}$ is the predicted output for the unlabeled input $x'$
\item $K: \mathcal{X}\times\mathcal{X} \mapsto \mathbb{R}$ is the kernel function
\item $\{(x_i, y_i)\}_{i=1}^{n}$ are the pre-trained examples with $y_i\in\{-1,+1\}\forall i$.
\item $w_i\in \mathbb{R}$ are the weights for the pre-trained examples. They are determined by the larning algorithm.
\item $\text{sgn}$ is the sign function.
\end{itemize}
With this example, we can clearly see how a kernel function can be used to label a new input. The kernel method is the learning algorithm that will provide the weights $w_i$.

\subsection{Kernel methods as convex optimization problems}

From section \ref{meth}, we already understand that the learning algorithm will have to find the best weights in order to have the most accurate prediction. The idea of the learning algorithm is to take a subset of the pre-trained examples, test the predictions, change the weight, etc. This can easily be seen as an optimization problem.  
\\\\

{\color{red} Je suis un peu perdu sur cette partie... }

\subsection{From infinite to finite}

{\bf \color{red} KERNEL TRICK + REPRESENTER THEOREM}

We saw in the section \ref{map}, we can use a feature map to go from the input space $\mathcal{X}$ to a Hilbert space of a very high or infinite dimension $\mathcal{H}$. However, having the explicit representation of the mapping can be difficult and it is not convenient to work with it because of the high dimension of $\mathcal{H}$. Therefore, the idea is to work implicitly in the feature space $\mathcal{H}$. In order to do this, we must define the kernel trick.

\begin{definition} [The Kernel trick]
We take any algorithm to process finite-dimensional vectors. If we can express this algorithms only in terms of pairwise inner products. Then this algorithm can be applied to potentially infinite-dimensional vectors in the feature space of a positive definite kernel if we replace each inner product evaluation by a kernel evaluation.
\end{definition}

Intuitively, the idea is that we will compute $K(x,x') = \langle\phi(x),\phi(x')\rangle_{\mathcal{H}}$ without even knowing the function $\phi(x)$ because it is often really complex.
\\\\



\section{Support vector machines: a special case}
\subsection{Generalities}
\subsection{Dual problem}
\subsection{A use case}
\end{document}
