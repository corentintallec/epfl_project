\documentclass[a4paper, 11pt]{article}
% pour l'inclusion de figures en eps,pdf,jpg
\usepackage{graphicx}
% quelques symboles mathematiques en plus
\usepackage{amsmath}
% le tout en langue francaise
%\usepackage[french]{babel}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc} 
\usepackage[T1]{fontenc}
% on utilise les liens internet
\usepackage{url}
% pour l'inclusion de links dans le document 
\usepackage[colorlinks,bookmarks=false,linkcolor=blue,urlcolor=blue]{hyperref}

\usepackage{tikz}
\usepackage{footnote}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{pgfplots}
\usepackage{listings}
\usepackage{array,hhline}
\usepackage{pgf}
\usepackage{multirow,graphicx,array,xcolor,colortbl}

\renewcommand*{\arraystretch}{1.5}

\newcommand{\cca}[2]{
	\cellcolor{black!#1}
	\ifnum #1>50
		\color{white}
	\fi
	{#2}
	}

\usepackage{float}

\usetikzlibrary{patterns,arrows,decorations.pathreplacing}


\usepackage{amsfonts}
\usepackage{amssymb}

\usepackage{verbatim}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% ATTENTION : package supplementaire
\usepackage[stable]{footmisc}
\usepackage{wrapfig}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paperheight=297mm
\paperwidth=210mm

\setlength{\textheight}{235mm}
\setlength{\topmargin}{-1.2cm} % pour centrer la page verticalement
%\setlength{\footskip}{5mm}
\setlength{\textwidth}{15cm}
\setlength{\oddsidemargin}{0.56cm}
\setlength{\evensidemargin}{0.56cm}
\setlength{\parindent}{0cm}

\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}

\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\K}{\bold{K}}
\newcommand{\Hilb}{\mathcal{H}}

\newcommand{\x}{x_i}
\newcommand{\y}{y_i}
\newcommand{\pmset}{\left\{\pm1\right\}}

\newcommand{\inner}[2]{\langle #1 , #2 \rangle_{\R^d}}
\newcommand{\innerh}[2]{\langle #1 , #2 \rangle_{\Hilb}}
\newcommand{\norm}[1]{\left\| #1 \right\|}
\newcommand{\normh}[1]{\left\| #1 \right\|_\Hilb}

\title{{\bf Kernel Methods and SVMs} \\
{\large A convex optimization point of view}}
\author{Gael Lederrey\\
  \and
Corentin Tallec}

\begin{document}
\maketitle
\begin{abstract}
Kernel methods, and more specifically support vectors machines, are
  well known, well theorized and efficient machine learning tools. For
  years, they were presenting state of the art performances in many
  machine learning fields, before being overtaken by deep learning
  methods. They have the enormous advantage of being well understood
  theoretically, and most interestingly to belong to the class of
  convex optimization problems, which is clearly not the case of the
  latter.

  In this document, we aim at presenting the general framework of
  Kernel methods, to expose how they relate to convex optimization,
  and how their optimization can be undertaken in practice. Besides,
  we intend to give a more precise treatment of the support vectors
  machines special case.
\end{abstract}

\section{Kernel methods\footnotemark}
\subsection{Generalities}
\label{sub:gen}

\footnotetext{The different elements presented in this section comes from the course on Machine Learning \cite{course} as well as the paper explaining how convex optimization is used with Kernel methods\cite{paper}.}

Kernel methods are tools broadly used in various fields, but most notably in
machine learning and pattern discrimination. Among this class of methods,
support vector machines are probably the best known kernel method. This class
of algorithm is used to find and study some types of relations in datasets,
{\it e.g.} classifications of the data, correlations between them, etc. Many
machine learning algorithms rely on the idea of embedding the data at hand
into a vector space that makes it easier analyse. Kernel methods provide
a quite general framework for doing so, and provide tools to transport data
into possibly infinite dimensional spaces. Kernel methods rely on a measure
of similarity between input data known as a {\bf kernel}, which has to be selected
beforehand.

A typical case of application for a kernel method is the following. Define
$\X$ (resp. $\Y$) as a set of datapoints (resp. a set of labels). Given
a training set, that is a set of couples $(\x, \y)_{i\leq n} \in (\X \times \Y)^n$,
we want to find an $f : \X \mapsto \Y$ in a particular set of functions $\F$ such
that $f$ predicts accurately labels in the training set, and is able to generalize
to unseen datapoints. One way of doing so is to solve the following optimization
problem:
\begin{equation}
  \min\limits_{f\in\F} \frac{1}{n}\sum\limits_{i=1}^nL(\y,f(\x)) + \lambda\Omega(f)
  \label{eq:min}
\end{equation}
where $L$ is a cost function, that measure how close $f(\x)$ is to $\y$ for
each $i$, $\lambda$ is a positive real number and $\Omega$ is a regularizing
term, that aims at describing the complexity of a certain function $f$. In the
general case, this optimization problem is not required to be convex. Kernel
methods provide a set of function $\F$ and a regularizing term $\Omega(f)$ such
that when the cost function is assumed to be convex in its second argument, the entire problem is
convex, and can be easily solved.

In the following sections, kernel functions are defined, as well as their
relation to the idea of feature mapping, and the general principle of kernel
methods is exposed.

\subsubsection{Positive Definite Kernel}
\label{subsub:psd}
Denote by $\X$ an arbitrary input set (notably $\X$ is not required to be a vector
space). A kernel can be seen as a measure of similarity between two elements of $\X$.
Formally
\begin{definition}
  A positive definite (p.d.) kernel on $\X$ is a function $K:\X\times\X \mapsto \R$ that
  is symmetric and that satisfies for all $N \in \N$, $(x_1, \dots, x_n) \in \X^N$,
  $(a_1, \dots, a_N) \in \R^\N$:
  \begin{equation}
    \sum\limits_{1 \leq i, j \leq N}a_ia_jK(x_i,x_j) \geq 0.
  \end{equation}
\end{definition}
If $\X = R^d$, the simplest kernel that can be thought of
is the canonical inner product. It obviously is symmetric, 
and positivity is easily verified. 
Under the same assumptions, it can quite easily be shown
that $K(x, x') = (\inner{x}{x'})^p$ is a kernel too (known
as the polynomial kernel). Another well known kernel is 
the gaussian kernel, defined as $K(x, x') = e^{- \frac{\norm{x-y}^2}{2\sigma^2}}$.

\subsubsection{Feature mapping}
\label{subsub:map}
As seen in Subsection~\ref{subsub:psd}, the canonical inner
product of a vector space is a kernel. Mercer's theorem provides
a kind of reciprocal statement: any kernel can be viewed as an
inner product in a certain hilbert space which is a functional
space on $\mathcal{X}$.
\begin{theorem} [Mercer's theorem]
The function $K$ is a positive definite kernel on $\mathcal{X}$ if and only
if there exists a Hilbert space $\mathcal{H}$ with an inner product $\langle
\cdot,\cdot\rangle_{\mathcal{H}}$ and a mapping
\begin{equation}
  \phi: \mathcal{X}\mapsto\mathcal{H}
\end{equation}
such that, for any $x$, $x'$ in $\mathcal{X}$:
\begin{equation}
  K(x,x') = \langle\phi(x),\phi(x')\rangle_{\mathcal{H}}
\end{equation}
\end{theorem}
Using a kernel can thus somehow be viewed as embedding the datapoints in
a (larger) space. One benefit of doing so can be, for example, to simplify
regression or classification problems, as these problems tend to
be easier in high dimensional space than in low dimensional ones.

\subsubsection{Kernel methods as convex optimization problems}
Recall the kind of problem exposed in Sec~\ref{sub:gen}, by eq~\ref{eq:min}.
Now, once a kernel $K$ is defined, it implicitly defines a functional space $\Hilb$ on $\X$, as
explained in Sec~\ref{subsub:map}. This space can be used as the prediction function space,
that is $\F = \Hilb$, and the regularization term can be replaced by the squared norm in $\Hilb$.

This gives the following minimization problem:
\begin{equation}
  \min\limits_{f\in\Hilb} \frac{1}{n}\sum\limits_{i=1}^nL(\y,f(\x)) + \lambda\normh{f}^2.
  \label{eq:minkernel}
\end{equation}
When $L$ is a convex function over its second argument, it is quite clear that the resulting minimization problem is
a convex optimization problem: the squared norm is a convex function of $f$, $f \mapsto f(x)$
is a linear function for any $x \in \X$, thus $L(\y,f(\x))$ is convex for all $i$, which lead
to the global convexity. However, the space $\Hilb$ considered is, in the general case, infinite,
and this makes convex optimization unapplicable.

\subsection{From infinite to finite}
Happily enough, the representer theorem stated below turns this infinite dimensional convex optimization
problem into your gentle everyday $n$-dimensional convex optimization problem:
\begin{theorem}[Representer theorem]
  Using the notations previously defined, let $\Psi:\R^{n+1}\mapsto\R$ be a function strictly increasing in 
  the last variable. Then any solution of
  \begin{equation}
    \min\limits_{f\in\Hilb}\Psi(f(x_1), \dots, f(x_n), \normh{f})
  \end{equation}
  admits a representation of the form:
  \begin{equation}
    \forall x \in \X, f(x) = \sum\limits_{i=1}^n\alpha_iK(\x,x).
    \label{eq:bestfit}
  \end{equation}
\end{theorem}
The equation~\ref{eq:minkernel} clearly falls in the field of application of the representer theorem.
The optimization problem can thus be rewritten in term of $\alpha$'s as:
\begin{equation}
  \min\limits_{(\alpha_1, \dots, \alpha_n)\in\R^n} 
  \frac{1}{n}\sum\limits_{i=1}^nL(\y, \sum\limits_{j=1}^n\alpha_jK(x_j, x_i)) +
  \lambda\sum\limits_{1\leq i,j\leq n}\alpha_i\alpha_j K(\x, x_j)
\end{equation}
which is a finite dimensional convex optimization problem.

\section{Support vector machines: a special case\footnotemark}
\footnotetext{The different elements presented in this section comes from the course on Machine Learning \cite{course} as well as the article on Wikip\'edia on SVM\cite{svm}.}
Support vector machines are kernel methods used in the field of binary
classification, and that can be extended (quite painfully, unfortunately) to
$n$-ary classification.  Among kernel methods, they are probably the best
known.  This might be explained by the fact that they enforce sparsity in the
best fit representation. Formally, they ensure that in eq~\ref{eq:bestfit}, a
varying number of $\alpha$'s will be exactly zero.  Sparsity often brings along
a lot of nice properties. For SVMs, most notably, they ensure a better
regularization (intuitively, a fit with less term is less complex than a fit
with many terms, and the number of term is directly related to the number of
non-zero $\alpha$'s), and they induce smaller computational cost. Indeed, the
best fit represented in eq~\ref{eq:bestfit} has as many terms as there are
datapoints in the train dataset. This means that the computational cost at test
time will depend directly on the number of datapoints in the training set. This
might prove computationally costly. SVMs partly solve this problem by ensuring
that some (most) $\alpha$'s are zero, and thus that the best fit will only
depend on a few training datapoints. 
\subsection{Generalities}
Given an input set $\X$, a p.s.d kernel on $\X$, $K$ and a training dataset,
$((x_1, y_1), \dots, (x_n, y_n)) \in (\X \times \pmset)^n$, SVMs solve the
kernel optimization problem eq~\ref{eq:minkernel} with the particular choice of
loss $L(x, y) = \max(0, 1 - x^\top y)$, which is known as the Hinge Loss.

A few remarks might be of use at this point:
\begin{itemize}
  \item The Hinge Loss is clearly convex in $x$. It is a maximum over two affine
    functions of $x$, and is thus convex.
  \item Datapoints are classified simply by looking at the sign of $f(x)$ where
    $f$ is the fit. If $f(x) > 0$, $x$ is classified as $+1$, else $x$ is classified
    ad $-1$. The specificity of the Hinge Loss is to try and drive $f(x)$ towards $+1$
    when $x$ is in the $+1$ class, and toward $-1$ when $x$ is in the $-1$ class, but not
    to do any further effort.
\end{itemize}
Rewritting eq~\ref{eq:minkernel} in the SVM case, and introducing
additionnal slack variables, the optimization problem we intend to
solve takes the form:
\begin{equation}
  \min\limits_{f\in\Hilb,\xi\in\R^n}
  \frac{1}{n}\sum\limits_{i=1}^n\xi_i + \lambda \normh{f}^2
\end{equation}
subject to:
\begin{equation}
  \begin{cases}
    \xi_i \geq 1 - y_i f(x_i)\\
    \xi_i \geq 0
  \end{cases}
\end{equation}
Using the representer theorem, this can be transformed in
\begin{equation}
  \min_{\alpha\in\R^n, \xi\in\R^n}
  \frac{1}{n}\sum\limits_{i=1}^n\xi_i
  + \lambda \alpha^\top \K \alpha
\end{equation}
subject to:
\begin{equation}
  \begin{cases}
    y_i\sum\limits_{j=1}^n\alpha_jK(\x, x_j) + \xi_i - 1 \geq 0\\
    \xi_i \geq 0
  \end{cases}
\end{equation}
where $\K$ is the p.s.d. matrix associated to the kernel $K$ on
the dataset, that is $\K_{i,j} = K(\x, x_j)$. This is a quadratic
program.

The dual problem can easily be expressed:
\begin{equation}
  \max\limits_{0\leq\mu\leq 1/n}
  \sum\limits_{i=1}^n\mu_i
  - \frac{1}{4\lambda}\sum\limits_{1\leq i,j \leq n}y_iy_j\mu_i\mu_jK(x_i, x_j).
\end{equation}
Now expressing the relation between primal and dual solutions:
\begin{equation}
  \alpha = \text{diag}(y)\mu/2\lambda
\end{equation}
(this is done by rewriting the optimization condition on the
Lagrangian with respect to the $\alpha$ variables)
and injecting this relation into the dual, we get the following
easier problem:
\begin{equation}
  2\alpha^\top y - \alpha^\top \K \alpha
\end{equation}
subject to:
\begin{equation}
  0 \leq y_i\alpha_i \leq \frac{1}{2\lambda n}
\end{equation}
(The crucial point in deriving this new optimization problem is
to notice that $y_i^2 = 1$, since $y_i \in \pmset$).

On this very simple optimization problem, the KKT conditions can
be easily expressed, and they lead to the following complementary
slackness equations:
\begin{equation}
  \begin{cases}
    \alpha_i\left[y_if(x_i) + \xi_i - 1\right] = 0\\
    \left[\alpha_i - \frac{y_i}{2\lambda n}\right]\xi_i = 0.
  \end{cases}
\end{equation}
With a bit of analysis with these equations and the constraints
of the primal, it can be easily shown that the only terms for
which $\alpha_i \neq 0$ are the examples ``hard to classify'' that
is for which $y_i f(x_i) \leq 1$ (those examples can be well
classified, but their classifiction margin is not large). All in
all, this tends to express the fact that only a few datapoints
are involved in the best fit expression. This has a tremendous
impact on further computations involving the best fit.

\subsection{Use case}

The theory works well, but a use case is always needed. We decided to use the SVM to learn our computer how to recognize handwritten digits. To do so, we create a script in python using the {\it sklearn}\cite{python} library. We used the MNIST database of handwritten digits\cite{mnist} provided by Yann Lecun and well studied. Each of the images (a handwritten digit between 0 and 9) has a given label. The purpose is to create a model using SVM such that the computer can recognize these digits. Some examples of the MNIST dataset are given in Figure \ref{fig:train-set}. 

\begin{figure}[H]
\centering
\input{./figures/train-set.tex}
\caption{\label{fig:train-set} Some examples of the MNIST train set with their label.}
\end{figure}

The purpose of this script was to train the computer on 60000 images with their labels. And then, test the model on 10000 images. We could finally compare the predicted labels against the real labels. In order to test the SVM method, we trained the algorithm with different kernel functions:
\begin{itemize}
\item Linear kernel: $K(x_i,x_j) = \langle x_i, x_j \rangle$
\item Polynomial kernel: $K(x_i,x_j) = (\gamma \langle x_i, x_j \rangle + r)^p$, $\gamma$ is a parameter that needs to be found and we decided to keep $r=0$ and we used $p=4$.
\item RBF kernel:  $K(x_i,x_j) = \exp(-\gamma|x_i-x_j|^2)$, $\gamma$ is a parameter that needs to be found.
\end{itemize} 

The most important result is the correctness of the prediction. To do this, we take the number of correct prediction and we divide it by the length of the dataset. This gives us a percentage on the correctness of the model. We give in Table \ref{tab:correct} the results for each of the kernels.

\begin{table}[H]
\centering
\begin{tabular}{l|c}
Kernels & Correct prediction \\
\hline
Linear & 94.62\% \\
Polynomial (degree 4) & 97.31\% \\ 
RBF & 98.56 \% \\ 
\end{tabular}
\caption{\label{tab:correct} Percentage of correct prediction for each of the kernels.}
\end{table}

As we can see, the best kernel is the RBF one. It has already been shown in the literature that this kernel is the state-of-the-art in SVM. Another important aspect is to have a look at the predictions more precisely. For this, we use a confusion matrix. This matrix shows the labels (vertical axis) in function of the predictions (horizontal axis). It is interesting to have a look at this matrix to see if there is some bias. (For example, 1 can be often predicted as 7). The confusion matrix for the RBF kernel is given in Table \ref{tab:conf_mat}. As we can see in this table, we don't have any bias. We just show some wrong predictions in Figure \ref{fig:wrong-label}. This figure shows us that some of the images are difficult to label, even for us.


\begin{table}[h]
\centering
\begin{tabular}{cc|c|c|c|c|c|c|c|c|c|c|}
\multicolumn{2}{c}{} & \multicolumn{10}{c}{Predictions} \\
\multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{3} & \multicolumn{1}{c}{4} & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{6} & \multicolumn{1}{c}{7} & \multicolumn{1}{c}{8} & \multicolumn{1}{c}{9}  \\
\cline{3-12}
\multirow{10}{*}{\rotatebox[origin=c]{90}{Labels}}
& 0 & \cca{99}{974} & 0 & 1 & 0 & 0 & 2 & 0 & 1 & 2 & 0 \\ \cline{3-12}
& 1 & 0 & \cca{99}{1128} & 3 & 1 & 0 & 1 & 0 & 1 & 1 & 0 \\ \cline{3-12}
& 2 & 4 & 0 & \cca{98}{1017} & 0 & 1 & 0 & 0 & 7 & 3 & 0 \\ \cline{3-12}
& 3 & 0 & 0 & 2 & \cca{99}{997} & 1 & 2 & 0 & 4 & 3 & 1 \\ \cline{3-12}
& 4 & 0 & 0 & 2 & 0 & \cca{99}{968} & 0 & 4 & 0 & 1 & 7 \\ \cline{3-12} 
& 5 & 2 & 0 & 0 & 5 & 1 & \cca{98}{877} & 3 & 1 & 2 & 1 \\ \cline{3-12} 
& 6 & 3 & 2 & 0 & 0 & 2 & 2 & \cca{99}{948} & 0 & 1 & 0 \\ \cline{3-12} 
& 7 & 0 & 3 & 8 & 1 & 1 & 0 & 0 & \cca{98}{1007} & 1 & 7 \\ \cline{3-12} 
& 8 & 3 & 0 & 1 & 3 & 1 & 1 & 0 & 2 & \cca{97}{959} & 4 \\ \cline{3-12} 
& 9 & 2 & 3 & 1 & 6 & 6 & 2 & 1 & 5 & 1 & \cca{100}{982} \\ \cline{3-12} 
\end{tabular}
\caption{\label{tab:conf_mat} Confusion Matrix for the RBF kernel.}
\end{table}

\begin{figure}[H]
\centering
\input{./figures/wrong-label.tex}
\caption{\label{fig:wrong-label} Examples of images that have not been labelled correctly. This number above the pictures are like this: label $\rightarrow$ prediction.}
\end{figure}

\section{Conclusion}

Within this project, we explored a really interesting topic. Indeed, Machine Learning as well as Artificial Intelligence is today's future technology. A lot of start-ups are working on this and applying it to a lot of different fields such as Facial and Emotion Recognition, Language Recognition, Data Mining, etc. Until not so long ago, the SVM were the state-of-the-art. Now, Random Forests and Deep-Learning (Convolutional Neural Networks) are better than this method. But their underlying theory is sometimes not totally understood compared to the SVM. Indeed, Convex Optimization is very known field by many scientist. Developing a model using Convex Optimization to make the computer learn things, such as handwritten digits, is quite amazing.

\begin{thebibliography}{1}

\bibitem{course} \href{http://lear.inrialpes.fr/people/mairal/teaching/2015-2016/MVA/fichiers/mva_slides.pdf}{Slides of the course on "{\it Kernel Methods for Machine Learning}", Inria, Grenoble, France}

\bibitem{paper} Kim, Seung-Jean, et al. "Learning the kernel via convex optimization." {\it Acoustics, Speech and Signal Processing, 2008. ICASSP 2008. IEEE International Conference on.} IEEE, 2008.

\bibitem{svm} \href{https://en.wikipedia.org/wiki/Support_vector_machine}{Wikip\'edia - Support Vector Machine}

\bibitem{python} \href{Library {\it sklearn} for Machine Learning in Python. Functions on SVM/SVC.}{http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html}

\bibitem{mnist} \href{The MNIST Databse of handwritten digits. (Website of Yann Lecun)}{http://yann.lecun.com/exdb/mnist/}

\end{thebibliography}



\end{document}
